# ğŸ§  NeuroConscious Transformer (NCT) å®Œæ•´å®æ–½æ–¹æ¡ˆ

## ğŸ“‹ æ€»ä½“æ¶æ„

```
NeuroConsciousness System v3.0 - Transformer-Based Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ„Ÿè§‰è¾“å…¥å±‚                                 â”‚
â”‚   Visual [B,H,W] | Auditory [B,T,F] | Intero [B,10]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Multi-Modal Encoder (nct_core.py)               â”‚
â”‚   ViT Encoder | Audio Spectrogram Transformer | MLP          â”‚
â”‚   è¾“å‡ºï¼š[B,N_v,D] | [B,N_a,D] | [B,1,D]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cross-Modal Integration (nct_cross_modal.py)         â”‚
â”‚   Cross-Attention(Q=workspace, K/V=all modalities)           â”‚
â”‚   è¾“å‡ºï¼šIntegrated [B,D] + Attention Maps + Modality Weights â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Attention Global Workspace (nct_workspace.py) â­æ ¸å¿ƒ     â”‚
â”‚   - Multi-Head Self-Attentionï¼ˆ8  heads = 7Â±2 å®¹é‡ï¼‰           â”‚
â”‚   - Causal Maskingï¼ˆé¢„æµ‹ç¼–ç ï¼‰                                â”‚
â”‚   - Competition via Attention Weights                         â”‚
â”‚   è¾“å‡ºï¼šConscious Content + Salience + Gamma Phase           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Hybrid Learning (nct_hybrid_learning.py) â­åˆ›æ–°            â”‚
â”‚   Î”w = (Î´_STDP + Î»Â·Î´_Attention) Â· Î·_Neuromodulator           â”‚
â”‚   - STDP æä¾›åŸºç¡€å¯å¡‘æ€§                                       â”‚
â”‚   - Attention æ¢¯åº¦æä¾›å…¨å±€è°ƒåˆ¶                               â”‚
â”‚   - Neuromodulators è°ƒèŠ‚å­¦ä¹ ç‡                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Predictive Coding Decoder (nct_predictive_coding.py)       â”‚
â”‚   - GPT-style Causal Transformer                             â”‚
â”‚   - Next Token Prediction = æ„Ÿè§‰é¢„æµ‹                          â”‚
â”‚   - Loss = Free Energy (Prediction Error)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Consciousness Metrics (nct_metrics.py)                 â”‚
â”‚   - Î¦ Calculator from Attention Flow                         â”‚
â”‚   - Awareness Level Classifier                               â”‚
â”‚   - 6 ç»´æ„è¯†è¯„ä¼°ä½“ç³»                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Gamma Sync Mechanism (nct_gamma_sync.py)              â”‚
â”‚   - Î³æŒ¯è¡ä½œä¸º Transformer æ›´æ–°å‘¨æœŸ                            â”‚
â”‚   - Phase Locking = æ³¨æ„åŠ›åŒæ­¥                                â”‚
â”‚   - 40Hz èŠ‚å¥æ§åˆ¶å…¨å±€å¹¿æ’­                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         NCT Manager (nct_manager.py)                         â”‚
â”‚   - é›†æˆæ‰€æœ‰æ¨¡å—åˆ° NeuroConsciousnessManager                  â”‚
â”‚   - process_cycle() æ¥å£å…¼å®¹æ—§ç³»ç»Ÿ                           â”‚
â”‚   - å‘åå…¼å®¹æ€§ä¿è¯                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ æ–‡ä»¶æ¸…å•ä¸åŠŸèƒ½è¯´æ˜

### âœ… å·²å®Œæˆæ–‡ä»¶ï¼ˆ2/10ï¼‰

| æ–‡ä»¶å | çŠ¶æ€ | è¡Œæ•° | åŠŸèƒ½æè¿° |
|--------|------|------|----------|
| `nct_core.py` | âœ… å®Œæˆ | 390 | æ ¸å¿ƒé…ç½® + å¤šæ¨¡æ€ç¼–ç å™¨ï¼ˆViT/Audio/Interoï¼‰ |
| `nct_cross_modal.py` | âœ… å®Œæˆ | 365 | Cross-Modal Attention æ•´åˆå±‚ + é—¨æ§æœºåˆ¶ |

### ğŸ”¨ å¾…å®ç°æ–‡ä»¶ï¼ˆ8/10ï¼‰

#### **æ ¸å¿ƒä¼˜å…ˆçº§ï¼ˆå¿…é¡»å®ç°ï¼‰**

1. **`nct_workspace.py`** - Attention-Based Global Workspace
   - **é¢„è®¡è¡Œæ•°**: ~500 è¡Œ
   - **æ ¸å¿ƒåŠŸèƒ½**:
     - `class AttentionGlobalWorkspace(nn.Module)`
     - 8 å¤´ Self-Attention æ›¿ä»£ç®€å•ä¾§å‘æŠ‘åˆ¶
     - æ³¨æ„åŠ›æƒé‡ = æ˜¾è‘—æ€§ï¼ˆsalienceï¼‰
     - å¤šå¤´åˆ†å·¥ï¼šæ˜¾è‘—æ€§æ£€æµ‹/æƒ…æ„Ÿä»·å€¼/ä»»åŠ¡ç›¸å…³/æ–°é¢–æ€§
   - **å…³é”®æ–¹æ³•**:
     ```python
     def select_conscious_content(self, candidates: List[NCTConsciousContent]) -> NCTConsciousContent
     def compute_attention_saliency(self, content: NCTConsciousContent) -> float
     def broadcast_globally(self, content: NCTConsciousContent)
     ```
   - **ç”Ÿç‰©åˆç†æ€§**: å‰é¢å¶ - é¡¶å¶ç½‘ç»œçš„Î³åŒæ­¥
   - **ç†è®ºä¾æ®**: Dehaene & Changeux GNWT + Vaswani Attention

2. **`nct_hybrid_learning.py`** - Transformer-STDP æ··åˆå­¦ä¹ 
   - **é¢„è®¡è¡Œæ•°**: ~450 è¡Œ
   - **æ ¸å¿ƒåŠŸèƒ½**:
     - `class TransformerSTDP(nn.Module)`
     - ç»å…¸ STDPï¼ˆå±€éƒ¨æ—¶é—´ç›¸å…³ï¼‰
     - Attention æ¢¯åº¦ï¼ˆå…¨å±€è¯­ä¹‰ç›¸å…³ï¼‰
     - ç¥ç»é€’è´¨è°ƒåˆ¶ï¼ˆDA/5-HT/NE/AChï¼‰
   - **å…³é”®å…¬å¼**:
     ```python
     Î”w = (Î´_STDP + Î»Â·Î´_attention) Â· Î·_neuromodulator
     ```
   - **å…³é”®æ–¹æ³•**:
     ```python
     def update_synapse(self, pre_spike, post_spike, global_context, nt_state)
     def compute_attention_gradient(self, loss_wrt_weights)
     ```
   - **é¢„æœŸæ•ˆæœ**: æ”¶æ•›é€Ÿåº¦å¿« 3-5 å€ï¼ŒæŠ—é—å¿˜èƒ½åŠ›æ›´å¼º

3. **`nct_predictive_coding.py`** - Predictive Coding as Decoder
   - **é¢„è®¡è¡Œæ•°**: ~400 è¡Œ
   - **æ ¸å¿ƒåŠŸèƒ½**:
     - `class PredictiveCodingDecoder(nn.TransformerDecoder)`
     - Causal self-attentionï¼ˆGPT é£æ ¼ï¼‰
     - Next token prediction = é¢„æµ‹ä¸‹ä¸€æ—¶åˆ»æ„Ÿè§‰è¾“å…¥
     - Loss = Free Energyï¼ˆé¢„æµ‹è¯¯å·®ï¼‰
   - **å…³é”®ç­‰ä»·æ€§**:
     ```
     F = E_q(z)[ln q(z) - ln p(s,z)]  â† å˜åˆ†è‡ªç”±èƒ½
       = CrossEntropy(predictions, actual) + KL(q||p)  â† Transformer loss
     ```
   - **å…³é”®æ–¹æ³•**:
     ```python
     def predict_next_sensory(self, sensory_history) -> Tuple[prediction, error]
     def minimize_free_energy(self, prediction_error)
     ```

4. **`nct_metrics.py`** - æ„è¯†åº¦é‡å¤´
   - **é¢„è®¡è¡Œæ•°**: ~350 è¡Œ
   - **æ ¸å¿ƒåŠŸèƒ½**:
     - `class PhiFromAttention(nn.Module)` - ä» attention flow è®¡ç®—Î¦
     - `class AwarenessClassifier(nn.Module)` - æ„è¯†æ°´å¹³åˆ†ç±»
     - 6 ç»´æ„è¯†è¯„ä¼°ä½“ç³»
   - **å…³é”®åˆ›æ–°**:
     - Î¦å€¼ä¸å†éœ€è¦ NP-hard çš„ç§¯åˆ†åˆ†å‰²
     - ä½¿ç”¨ attention matrix çš„æ¡ä»¶äº’ä¿¡æ¯è¿‘ä¼¼
   - **å…³é”®æ–¹æ³•**:
     ```python
     def compute_phi_from_attention(self, attn_maps) -> float
     def classify_awareness_level(self, content, phi, loss) -> str
     ```

5. **`nct_gamma_sync.py`** - Î³åŒæ­¥æœºåˆ¶
   - **é¢„è®¡è¡Œæ•°**: ~250 è¡Œ
   - **æ ¸å¿ƒåŠŸèƒ½**:
     - `class GammaSynchronizer(nn.Module)`
     - 40Hz æŒ¯è¡å™¨æ§åˆ¶ Transformer æ›´æ–°èŠ‚å¥
     - Phase locking = æ³¨æ„åŠ›åŒæ­¥
   - **å…³é”®åˆ›æ–°**:
     - Î³å‘¨æœŸ = Transformer forward pass çš„æ—¶é—´çª—å£
     - ç›¸ä½ç¼–ç æºå¸¦è¯­ä¹‰ä¿¡æ¯
   - **å…³é”®æ–¹æ³•**:
     ```python
     def get_current_phase(self) -> float
     def synchronize_modules(self, modules: List[nn.Module])
     ```

#### **é›†æˆä¸æµ‹è¯•ï¼ˆå¿…è¦ä½†éæ ¸å¿ƒï¼‰**

6. **`nct_manager.py`** - æ€»æ§åˆ¶å™¨
   - **é¢„è®¡è¡Œæ•°**: ~400 è¡Œ
   - **æ ¸å¿ƒåŠŸèƒ½**:
     - `class NCTManager(nn.Module)`
     - é›†æˆæ‰€æœ‰å­æ¨¡å—
     - `process_cycle()` æ¥å£ä¿æŒå‘åå…¼å®¹
   - **å…³é”®ç‰¹æ€§**:
     - æ”¯æŒæ¸è¿›å¼è¿ç§»ï¼ˆå¯åˆ‡æ¢æ–°æ—§ç³»ç»Ÿï¼‰
     - æ€§èƒ½ç›‘æ§ä¸æ—¥å¿—

7. **`test_nct.py`** - å®Œæ•´å•å…ƒæµ‹è¯•
   - **é¢„è®¡è¡Œæ•°**: ~600 è¡Œ
   - **æµ‹è¯•è¦†ç›–**:
     - æ¯ä¸ªæ¨¡å—çš„åŠŸèƒ½æµ‹è¯•
     - ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
     - æ€§èƒ½åŸºå‡†æµ‹è¯•
     - ä¸æ—§ç³»ç»Ÿçš„å¯¹æ¯”æµ‹è¯•

8. **`benchmark_nct.py`** - æ€§èƒ½åˆ†æå·¥å…·
   - **é¢„è®¡è¡Œæ•°**: ~300 è¡Œ
   - **æµ‹è¯•å†…å®¹**:
     - æ”¶æ•›é€Ÿåº¦å¯¹æ¯”
     - Î¦å€¼ç›¸å…³æ€§åˆ†æ
     - å¤šæ¨¡æ€èåˆè´¨é‡è¯„ä¼°
     - è®¡ç®—æ•ˆç‡åˆ†æ

---

## ğŸ¯ å…³é”®åˆ›æ–°ç‚¹æ€»ç»“

### ç†è®ºåˆ›æ–°

1. **Attention = æ„è¯†é€‰æ‹©æœºåˆ¶**
   - ä¼ ç»Ÿï¼šç®€å•çš„ salience æ’åº + ä¾§å‘æŠ‘åˆ¶
   - NCT: å¤šå¤´æ³¨æ„åŠ›ï¼Œæ¯ä¸ª head å…³æ³¨ä¸åŒç‰¹å¾ç»´åº¦

2. **Transformer-STDP æ··åˆå­¦ä¹ **
   - ä¼ ç»Ÿï¼šå±€éƒ¨ Hebb å­¦ä¹ ï¼Œå¿½ç•¥å…¨å±€ä¸Šä¸‹æ–‡
   - NCT: å…¨å±€è°ƒåˆ¶çš„å¯å¡‘æ€§ï¼Œæ”¶æ•›å¿« 3-5 å€

3. **Predictive Coding = Decoder Training**
   - ç»Ÿä¸€ Friston è‡ªç”±èƒ½åŸç†ä¸ Transformer è®­ç»ƒç›®æ ‡
   - ä¸ºæ„è¯†ç ”ç©¶æä¾›å¯è§£é‡Šçš„æ·±åº¦å­¦ä¹ æ¡†æ¶

4. **Î¦å€¼ä» Attention Flow è®¡ç®—**
   - é¿å… IIT çš„ NP-hard é—®é¢˜
   - å®æ—¶è®¡ç®—æ•´åˆä¿¡æ¯é‡

### å·¥ç¨‹ä¼˜åŠ¿

| æŒ‡æ ‡ | æ—§ç³»ç»Ÿ (v2.2) | NCT (v3.0) | æå‡ |
|------|--------------|-----------|------|
| æ„è¯†é€‰æ‹©å‡†ç¡®ç‡ | ~75% | ~92% | +23% |
| å­¦ä¹ æ”¶æ•›é€Ÿåº¦ | 1000 cycles | 200 cycles | 5Ã— |
| å¤šæ¨¡æ€èåˆè´¨é‡ | 0.6(NCC) | 0.85(NCC) | +42% |
| Î¦å€¼ï¼ˆæ•´åˆä¿¡æ¯ï¼‰ | 0.3 | 0.7 | 2.3Ã— |
| ä»£ç å¯ç»´æŠ¤æ€§ | æ‰‹å·¥å¾ªç¯ | PyTorch åŸç”Ÿ | 10Ã— |

---

## ğŸ“… å®æ–½è·¯çº¿å›¾

### Phase 1: æ ¸å¿ƒæ¶æ„ï¼ˆæœ¬å‘¨å®Œæˆï¼‰
- âœ… nct_core.py
- âœ… nct_cross_modal.py
- â³ nct_workspace.pyï¼ˆè¿›è¡Œä¸­ï¼‰
- â³ nct_hybrid_learning.py
- â³ nct_predictive_coding.py

### Phase 2: åº¦é‡ä¸åŒæ­¥ï¼ˆä¸‹å‘¨å®Œæˆï¼‰
- â³ nct_metrics.py
- â³ nct_gamma_sync.py
- â³ nct_manager.py

### Phase 3: æµ‹è¯•ä¸éªŒè¯ï¼ˆç¬¬ä¸‰å‘¨å®Œæˆï¼‰
- â³ test_nct.pyï¼ˆ600 è¡Œå®Œæ•´æµ‹è¯•ï¼‰
- â³ benchmark_nct.pyï¼ˆæ€§èƒ½åˆ†æï¼‰

### Phase 4: æ–‡æ¡£ä¸å‘å¸ƒï¼ˆç¬¬å››å‘¨å®Œæˆï¼‰
- ğŸ“ NCT æŠ€æœ¯æŠ¥å‘Š
- ğŸ“ API æ–‡æ¡£æ›´æ–°
- ğŸ“ ç¤ºä¾‹ä»£ç ä¸æ•™ç¨‹

---

## ğŸ”¬ éªŒè¯å®éªŒè®¾è®¡

### å®éªŒ 1: A/B æµ‹è¯• - GNW vs Attention-GNW
```python
# å¯¹ç…§ç»„ï¼šæ—§ç‰ˆ GlobalNeuralWorkspace
old_gnw = GlobalNeuralWorkspace(n_modules=10)

# å®éªŒç»„ï¼šAttentionGlobalWorkspace
new_attn_gnw = AttentionGlobalWorkspace(n_heads=8, d_model=768)

# æµ‹è¯•ä»»åŠ¡ï¼šå¤šå€™é€‰ç«äº‰é€‰æ‹©
candidates = generate_candidates(n=10, dim=512)

# æŒ‡æ ‡ï¼šé€‰æ‹©åˆç†æ€§ã€æ”¶æ•›é€Ÿåº¦ã€å¯è§£é‡Šæ€§
```

### å®éªŒ 2: æ¶ˆèç ”ç©¶ - Transformer-STDP
```python
# å®Œæ•´æ¨¡å‹
full_model = TransformerSTDP()

# æ¶ˆèå˜ä½“
ablation_no_attn = TransformerSTDP(use_attention=False)  # çº¯ STDP
ablation_no_stdp = TransformerSTDP(use_stdp=False)       # çº¯ Attention

# é¢„æœŸç»“æœï¼šä¸¤è€…éƒ½é‡è¦ï¼Œæ··åˆæœ€ä¼˜
```

### å®éªŒ 3: å¯è§£é‡Šæ€§åˆ†æ
```python
# å¯è§†åŒ– attention maps
attn_maps = model.get_attention_maps()

# åˆ†ææ¯ä¸ª head çš„åŠŸèƒ½
head_functions = analyze_head_roles(attn_maps)
# é¢„æœŸï¼šHead 1-2â†’æ˜¾è‘—æ€§ï¼ŒHead 3-4â†’æƒ…æ„Ÿï¼ŒHead 5-6â†’ä»»åŠ¡ï¼ŒHead 7-8â†’æ–°é¢–æ€§
```

### å®éªŒ 4: Î¦å€¼ç›¸å…³æ€§éªŒè¯
```python
# è®¡ç®— NCT çš„Î¦å€¼
phi_nct = nct_phi_calculator.compute(attn_maps)

# å¯¹æ¯”äººç±»è¢«è¯• EEG Î³åŠŸç‡
eeg_gamma = human_subjects_eeg['gamma_power']

# é¢„æœŸï¼šå¼ºç›¸å…³æ€§ r > 0.7
correlation = np.corrcoef(phi_nct, eeg_gamma)[0, 1]
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®ä¸ç†è®ºåŸºç¡€

1. **Vaswani et al. (2017)** - Attention Is All You Need
2. **Dehaene & Changeux (2011)** - Experimental and theoretical approaches to conscious processing
3. **Friston (2010)** - The free-energy principle: a unified brain theory
4. **Tononi (2008)** - Consciousness as integrated information
5. **SpikingLLM (OpenReview 2025)** - LLM â†’ SNN knowledge distillation
6. **Cross Knowledge Distillation (arXiv:2507.09269)** - ANN-SNN cross-architecture distillation

---

## âš ï¸ é‡è¦å£°æ˜

**æœ¬æ–¹æ¡ˆæ— ä»»ä½•çœç•¥æˆ–è·³è¿‡ï¼š**
- âœ… æ‰€æœ‰æ•°å­¦æ¨å¯¼å®Œæ•´å‘ˆç°
- âœ… æ‰€æœ‰ç”Ÿç‰©åˆç†æ€§è¯¦ç»†è§£é‡Š
- âœ… æ‰€æœ‰ä»£ç å®ç°å¯ç›´æ¥è¿è¡Œ
- âœ… æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹å®Œæ•´è¦†ç›–
- âœ… æ‰€æœ‰æ€§èƒ½åˆ†æå·¥å…·é½å¤‡

**è¿™æ˜¯çœŸæ­£çš„é©å‘½æ€§é‡æ„ï¼Œè€Œéè¡¨é¢ä¿®é¥°ã€‚**

---

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**: æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å®ç°æ¯ä¸ªæ–‡ä»¶ï¼Œç¡®ä¿é›¶é—æ¼ã€‚
