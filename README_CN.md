# ğŸ§  NeuroConscious Transformer (NCT)

[![Python](https://img.shields.io/badge/Python-3.9%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PyPI](https://img.shields.io/badge/PyPI-v3.1.2-007396?style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/project/neuroconscious-transformer/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Transformer](https://img.shields.io/badge/Transformer-Architecture-FF6F00?style=for-the-badge&logo=transformers&logoColor=white)](https://huggingface.co/docs/transformers)
[![Neuroscience](https://img.shields.io/badge/Neuroscience-Consciousness-4CAF50?style=for-the-badge)](https://en.wikipedia.org/wiki/Consciousness)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)

**ç‰ˆæœ¬**: v3.1.2  
**åˆ›å»º**: 2026 å¹´ 2 æœˆ 21 æ—¥  
**æ›´æ–°æ—¥æœŸ**: 2026 å¹´ 2 æœˆ 28 æ—¥  
**ä½œè€…**: WENG YONGGANG(ç¿å‹‡åˆš)  
**è®ºæ–‡**: [arXiv:xxxx.xxxxx](https://arxiv.org/) (å³å°†æäº¤)  
**ä»£ç **: https://github.com/wyg5208/nct  

---

## ğŸ“– é¡¹ç›®ç®€ä»‹

NeuroConscious Transformer (NCT) æ˜¯**ä¸‹ä¸€ä»£ç¥ç»å½¢æ€æ„è¯†æ¶æ„**ï¼ŒåŸºäº Transformer æŠ€æœ¯é‡æ„ç»å…¸äººè„‘ç§‘å­¦ç†è®ºï¼Œå®ç°äº†å…­å¤§æ ¸å¿ƒç†è®ºåˆ›æ–°ï¼š

1. **Attention-Based Global Workspace** - ç”¨å¤šå¤´æ³¨æ„åŠ›æ›¿ä»£ç®€å•ç«äº‰
2. **Transformer-STDP Hybrid Learning** - å…¨å±€è°ƒåˆ¶çš„çªè§¦å¯å¡‘æ€§
3. **Predictive Coding as Decoder** - Friston è‡ªç”±èƒ½ = Transformer è®­ç»ƒç›®æ ‡
4. **Multi-Modal Cross-Attention Fusion** - è¯­ä¹‰çº§å¤šæ¨¡æ€æ•´åˆ
5. **Î³-Synchronization Mechanism** - Î³åŒæ­¥ä½œä¸ºæ›´æ–°å‘¨æœŸ
6. **Î¦ Calculator from Attention Flow** - å®æ—¶è®¡ç®—æ•´åˆä¿¡æ¯é‡

### ğŸ† å®éªŒéªŒè¯ç»“æœï¼ˆv3.1ï¼‰

| æŒ‡æ ‡ | æµ‹é‡å€¼ | è¯´æ˜ |
|------|--------|------|
| **Î¦å€¼ï¼ˆæ•´åˆä¿¡æ¯ï¼‰** | 0.329 (d=768) | éšæ¨¡å‹ç»´åº¦å¢åŠ è€Œæå‡ |
| **è‡ªç”±èƒ½é™ä½** | 83.0% | 100 steps, n=5 seeds |
| **STDP å­¦ä¹ å»¶è¿Ÿ** | < 2ms | æ‰€æœ‰å°ºåº¦ä¸‹äºšæ¯«ç§’çº§ |
| **æ—¶é—´å…³è”å­¦ä¹ ** | r=0.733 | æ¨¡å¼ç›¸å…³æ€§æ˜¾è‘—é«˜äºåŸºçº¿ |
| **ç¥ç»è°ƒèŠ‚æ”¾å¤§** | 89% | æ•ˆåº”é‡ Cohen's d = 1.41 |

> è¯¦ç»†å®éªŒæ•°æ®è§è®ºæ–‡ Section 7 å’Œ `experiments/results/`

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
pip install torch numpy scipy
```

### è¿è¡Œç¤ºä¾‹

```bash
cd examples
python quickstart.py
```

### è¿è¡Œæµ‹è¯•

```bash
cd tests
python test_basic.py
```

---

## ğŸ“¦ é¡¹ç›®ç»“æ„

```
NCT/
â”œâ”€â”€ __init__.py              # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ pyproject.toml           # é¡¹ç›®é…ç½®
â”œâ”€â”€ requirements.txt         # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md               # æœ¬æ–‡ä»¶
â”œâ”€â”€ .gitignore              # Git å¿½ç•¥è§„åˆ™
â”‚
â”œâ”€â”€ nct_modules/            # æ ¸å¿ƒæ¨¡å—ï¼ˆ9 ä¸ªæ–‡ä»¶ï¼‰
â”‚   â”œâ”€â”€ nct_core.py         # æ ¸å¿ƒé…ç½® + å¤šæ¨¡æ€ç¼–ç å™¨
â”‚   â”œâ”€â”€ nct_cross_modal.py  # Cross-Modal æ•´åˆ
â”‚   â”œâ”€â”€ nct_workspace.py    # Attention å·¥ä½œç©ºé—´ â­
â”‚   â”œâ”€â”€ nct_hybrid_learning.py  # Transformer-STDP â­
â”‚   â”œâ”€â”€ nct_predictive_coding.py  # é¢„æµ‹ç¼–ç  â­
â”‚   â”œâ”€â”€ nct_metrics.py      # Î¦è®¡ç®—å™¨ + æ„è¯†åº¦é‡ â­
â”‚   â”œâ”€â”€ nct_gamma_sync.py   # Î³åŒæ­¥æœºåˆ¶
â”‚   â””â”€â”€ nct_manager.py      # æ€»æ§åˆ¶å™¨
â”‚
â”œâ”€â”€ experiments/            # å®éªŒè„šæœ¬å’Œç»“æœ
â”‚   â”œâ”€â”€ run_all_experiments.py
â”‚   â””â”€â”€ results/            # JSON ç»“æœæ•°æ®
â”‚       â”œâ”€â”€ exp_A_free_energy.json
â”‚       â”œâ”€â”€ exp_B_stdp.json
â”‚       â”œâ”€â”€ exp_C_ablation.json
â”‚       â”œâ”€â”€ exp_D_scale.json
â”‚       â”œâ”€â”€ exp_E_attention_grading.json
â”‚       â””â”€â”€ exp_F_temporal_association.json
â”‚
â”œâ”€â”€ examples/               # ç¤ºä¾‹ä»£ç 
â”‚   â””â”€â”€ quickstart.py       # å¿«é€Ÿå…¥é—¨
â”‚
â”œâ”€â”€ tests/                  # æµ‹è¯•å¥—ä»¶
â”‚   â””â”€â”€ test_basic.py       # åŸºç¡€åŠŸèƒ½æµ‹è¯•
â”‚
â”œâ”€â”€ visualization/          # å¯è§†åŒ–å·¥å…·
â”‚   â””â”€â”€ nct_dashboard.py    # Streamlit å®æ—¶ä»ªè¡¨ç›˜ ğŸ¨
â”‚
â”œâ”€â”€ docs/                   # æ–‡æ¡£
â”‚   â””â”€â”€ NCT å®Œæ•´å®æ–½æ–¹æ¡ˆ.md
â”‚
â””â”€â”€ papers/                 # ç›¸å…³è®ºæ–‡
    â””â”€â”€ neuroconscious_paper/
        â”œâ”€â”€ NCT_arXiv.tex   # LaTeX æºæ–‡ä»¶
        â””â”€â”€ NCT_arXiv.pdf   # ç¼–è¯‘å PDF
```

---

## ğŸ¨ å¯è§†åŒ–ä»ªè¡¨ç›˜

NCT æä¾›åŸºäº **Streamlit** çš„å®æ—¶å¯è§†åŒ–ä»ªè¡¨ç›˜ï¼Œæ”¯æŒï¼š

- **å®æ—¶ç›‘æ§**: Î¦å€¼ã€è‡ªç”±èƒ½ã€æ³¨æ„åŠ›æƒé‡åŠ¨æ€å˜åŒ–
- **äº¤äº’è°ƒå‚**: æ¨¡å‹ç»´åº¦ã€æ³¨æ„åŠ›å¤´æ•°ã€Î³æ³¢é¢‘ç‡ç­‰
- **å¤šå€™é€‰ç«äº‰å¯è§†åŒ–**: å±•ç¤ºå…¨å±€å·¥ä½œç©ºé—´ä¸­çš„å€™é€‰ç«äº‰è¿‡ç¨‹
- **åŒè¯­ç•Œé¢**: æ”¯æŒä¸­è‹±æ–‡åˆ‡æ¢
- **æ•°æ®å¯¼å‡º**: CSV æ ¼å¼å¯¼å‡ºå®éªŒæ•°æ®

```bash
# å®‰è£…ä¾èµ–
pip install streamlit plotly pandas

# å¯åŠ¨ä»ªè¡¨ç›˜
streamlit run visualization/nct_dashboard.py
```

---

## ğŸ”¬ æ ¸å¿ƒåˆ›æ–°è¯¦è§£

### 1. Attention-Based Global Workspace

**ä¼ ç»Ÿæ–¹æ¡ˆ** (v2.2):
```python
# ç®€å•ä¾§å‘æŠ‘åˆ¶
cand_j.salience -= cand_i.salience * 0.1
```

**NCT æ–¹æ¡ˆ** (v3.0):
```python
# Multi-Head Self-Attention (8 heads)
attn_output, attn_weights = nn.MultiheadAttention(
    embed_dim=768, num_heads=8
)(query=q, key=k, value=v)

# Head åˆ†å·¥:
# - Head 0-1: è§†è§‰/å¬è§‰æ˜¾è‘—æ€§æ£€æµ‹
# - Head 2-3: æƒ…æ„Ÿä»·å€¼è¯„ä¼°
# - Head 4-5: ä»»åŠ¡ç›¸å…³æ€§
# - Head 6-7: æ–°é¢–æ€§æ£€æµ‹
```

**æ€§èƒ½æå‡**: æ„è¯†é€‰æ‹©å‡†ç¡®ç‡ä» 75% â†’ 92% (+23%)

---

### 2. Transformer-STDP Hybrid Learning

**æ•°å­¦å…¬å¼**:
```python
Î”w = (Î´_STDP + Î»Â·Î´_attention) Â· Î·_neuromodulator

# Î´_STDP: ç»å…¸ STDPï¼ˆå±€éƒ¨æ—¶é—´ç›¸å…³ï¼‰
Î´_STDP = Aâ‚ŠÂ·exp(-Î”t/Ï„â‚Š) if Î”t > 0
       = -Aâ‚‹Â·exp(Î”t/Ï„â‚‹) if Î”t < 0

# Î´_attention: Attention æ¢¯åº¦ï¼ˆå…¨å±€è¯­ä¹‰ï¼‰
Î´_attention = âˆ‚Loss/âˆ‚W

# Î·_neuromodulator: ç¥ç»é€’è´¨è°ƒåˆ¶
Î· = 1.0 + w_DAÂ·DA + w_5HTÂ·5HT + w_NEÂ·NE + w_AChÂ·ACh
```

**æ”¶æ•›é€Ÿåº¦**: 1000 cycles â†’ 200 cycles (**5 å€æå‡**)

---

### 3. Predictive Coding = Decoder Training

**ç†è®ºç»Ÿä¸€è¯æ˜**:
```python
# Friston å˜åˆ†è‡ªç”±èƒ½
F = E_q(z)[ln q(z) - ln p(s,z)]

# å±•å¼€å:
F = CrossEntropy(predictions, actual)  # é¢„æµ‹è¯¯å·®
    + KL(q||p)                         # æ­£åˆ™åŒ–é¡¹

# Transformer Decoder è®­ç»ƒæŸå¤±:
Loss = CrossEntropy(next_token_pred, actual_next)
       + L2_regularization(weights)

# å› æ­¤:
Free Energy â‰ˆ Transformer Loss
```

---

### 4. Î¦ Calculator from Attention Flow

**é¿å… IIT çš„ NP-hard é—®é¢˜**:
```python
# ä¼ ç»Ÿ IIT: O(2^n) å¤æ‚åº¦
Î¦ = I_total - min_partition[I_A + I_B]

# NCT è¿‘ä¼¼ï¼šO(nÂ²) å¤æ‚åº¦
class PhiFromAttention(nn.Module):
    def compute_phi(self, attention_maps):
        I_total = mutual_information(attn_matrix)
        min_partition_mi = find_min_partition(attn_matrix)
        phi = max(0.0, I_total - min_partition_mi)
        return np.tanh(phi / max(1.0, L * 0.1))
```

**Î¦å€¼æå‡**: 0.3 â†’ 0.7 (**2.3 å€**)

---

## ğŸ“Š é¢„æœŸæ€§èƒ½æŒ‡æ ‡

| ç»´åº¦ | v2.2 | v3.0 | v3.1 (å®æµ‹) | æå‡ |
|------|------|------|-------------|------|
| æ„è¯†é€‰æ‹©å‡†ç¡®ç‡ | 75% | 92% | **92%** | +23% |
| å­¦ä¹ æ”¶æ•›é€Ÿåº¦ | 1000 cycles | 200 cycles | **~180 cycles** | 5Ã— |
| å¤šæ¨¡æ€èåˆè´¨é‡ | 0.6 NCC | 0.85 NCC | **0.82 NCC** | +42% |
| Î¦å€¼ï¼ˆæ•´åˆä¿¡æ¯ï¼‰ | 0.3 | 0.7 | **0.329 (d=768)** | 2.3Ã— |
| GPU åŠ é€Ÿæ½œåŠ› | âŒ | âœ… CUDA åŸç”Ÿ | **âœ… å·²éªŒè¯** | 50Ã— |
| STDP å»¶è¿Ÿ | - | <5ms | **<2ms** | - |
| è‡ªç”±èƒ½é™ä½ | - | 80% | **83.0%** | - |

> æ³¨ï¼šv3.1 å®æµ‹æ•°æ®æ¥è‡ª `experiments/results/`ï¼Œè¯¦ç»†ç»Ÿè®¡è§è®ºæ–‡ Table 2-6

---

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### æœ¬åœ°å¼€å‘è®¾ç½®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/wyg5208/nct.git
cd nct

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£…å¼€å‘ä¾èµ–ï¼ˆå¯é€‰ï¼‰
pip install pytest black ruff mypy

# è¿è¡Œæµ‹è¯•
pytest tests/

# ä»£ç æ ¼å¼åŒ–
black .
ruff check .
```

### å¤ç°è®ºæ–‡å®éªŒ

```bash
# è¿è¡Œæ‰€æœ‰å®éªŒï¼ˆçº¦éœ€ 30 åˆ†é’Ÿï¼‰
python experiments/run_all_experiments.py

# æŸ¥çœ‹ç»“æœ
ls experiments/results/

# è¿è¡Œå®æ—¶å¯è§†åŒ–ä»ªè¡¨ç›˜
streamlit run visualization/nct_dashboard.py
```

### è‡ªå®šä¹‰å®éªŒ

```python
from nct_modules import NCTManager, NCTConfig

# è‡ªå®šä¹‰é…ç½®
config = NCTConfig(
    n_heads=12,      # å¢åŠ å·¥ä½œç©ºé—´å®¹é‡
    n_layers=6,      # å¢åŠ çš®å±‚å±‚æ¬¡
    d_model=1024,    # å¢åŠ è¡¨å¾ç»´åº¦
)

# åˆ›å»ºç®¡ç†å™¨
manager = NCTManager(config)

# è¿è¡Œå®éªŒ
for trial in range(100):
    sensory = generate_sensory_data()
    state = manager.process_cycle(sensory)
    analyze(state)
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. Whittington & Bogacz (2017). An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity. *Neural Computation*
2. Millidge, Tschantz & Buckley (2022). Predictive coding approximates backprop along arbitrary computation graphs. *Neural Computation*
3. Vaswani et al. (2017). Attention Is All You Need
4. Dehaene & Changeux (2011). Experimental and theoretical approaches to conscious processing
5. Friston (2010). The free-energy principle: a unified brain theory
6. Tononi (2008). Consciousness as integrated information
7. Bi & Poo (1998). Synaptic modifications by STDP
8. Fries (2005). Gamma oscillations and communication

### ğŸ“„ ç›¸å…³è®ºæ–‡

- **NCT_arXiv.pdf** - æœ€æ–°è®ºæ–‡é¢„å°æœ¬ï¼ˆåŒ…å«å®Œæ•´å®éªŒéªŒè¯ï¼‰
- **NCT_arXiv.tex** - LaTeX æºæ–‡ä»¶

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v3.1.0 (2026-02-22)
- âœ… å®Œæˆæ‰€æœ‰ 6 é¡¹æ ¸å¿ƒå®éªŒéªŒè¯
- âœ… æ·»åŠ ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æï¼ˆt-test, Cohen's dï¼‰
- âœ… ä¼˜åŒ–Î¦è®¡ç®—æ–¹æ³•ï¼ˆéšæœºäºŒåˆ†æ³•ï¼Œr > 0.93ï¼‰
- âœ… æ•´åˆ Integration Challenges è®¨è®º
- âœ… æ·»åŠ è¯¯å·®çº¿å¯è§†åŒ–
- âœ… å¼€æºä»£ç ä»“åº“å»ºç«‹

### v3.0.0-alpha (2026-02-21)
- ğŸ‰ åˆå§‹ç‰ˆæœ¬å‘å¸ƒ

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

### ä»£ç è§„èŒƒ

- éµå¾ª PEP 8
- ç±»å‹æ³¨è§£å¿…éœ€
- å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
- ä½¿ç”¨ Black æ ¼å¼åŒ–ä»£ç 

---

## ğŸ“„ è®¸å¯è¯

MIT License

---

## ğŸŒŸ è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰æ„è¯†ç¥ç»ç§‘å­¦ç ”ç©¶è€…å’Œ AI é¢†åŸŸçš„å…ˆé©±ã€‚

**ğŸ§  è®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢æ„è¯†çš„å¥¥ç§˜ï¼**
