# üß† NeuroConscious Transformer (NCT)

[![Python](https://img.shields.io/badge/Python-3.9%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PyPI](https://img.shields.io/badge/PyPI-v3.1.4-007396?style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/project/neuroconscious-transformer/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Transformer](https://img.shields.io/badge/Transformer-Architecture-FF6F00?style=for-the-badge&logo=transformers&logoColor=white)](https://huggingface.co/docs/transformers)
[![Neuroscience](https://img.shields.io/badge/Neuroscience-Consciousness-4CAF50?style=for-the-badge)](https://en.wikipedia.org/wiki/Consciousness)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge)](LICENSE)

**Version**: v3.1.4  
**Created**: February 21, 2026  
**Updated**: February 28, 2026  
**Author**: WENG YONGGANG(ÁøÅÂãáÂàö)  
**Paper**: [arXiv:xxxx.xxxxx](https://arxiv.org/) (Forthcoming)  
**Code**: https://github.com/wyg5208/nct  

[‰∏≠ÊñáÊñáÊ°£](README_CN.md)

---

## üìñ Overview

NeuroConscious Transformer (NCT) is a **next-generation neuromorphic consciousness architecture** that reconstructs classical neuroscience theories using Transformer technology, achieving six core theoretical innovations:

1. **Attention-Based Global Workspace** - Replacing simple competition with multi-head attention
2. **Transformer-STDP Hybrid Learning** - Globally modulated synaptic plasticity
3. **Predictive Coding as Decoder** - Friston's free energy = Transformer training objective
4. **Multi-Modal Cross-Attention Fusion** - Semantic-level multimodal integration
5. **Œ≥-Synchronization Mechanism** - Gamma synchronization as update cycle
6. **Œ¶ Calculator from Attention Flow** - Real-time integrated information computation

### üèÜ Experimental Results (v3.1)

| Metric | Measured Value | Description |
|--------|----------------|-------------|
| **Œ¶ Value (Integrated Information)** | 0.329 (d=768) | Increases with model dimension |
| **Free Energy Reduction** | 83.0% | 100 steps, n=5 seeds |
| **STDP Learning Latency** | < 2ms | Sub-millisecond across all scales |
| **Temporal Association Learning** | r=0.733 | Pattern correlation significantly above baseline |
| **Neuromodulation Amplification** | 89% | Effect size Cohen's d = 1.41 |

> Detailed experimental data available in Paper Section 7 and `experiments/results/`

---

## üöÄ Quick Start

### Installation

```bash
pip install torch numpy scipy
```

### Run Examples

```bash
cd examples
python quickstart.py
```

### Run Tests

```bash
cd tests
python test_basic.py
```

---

## üì¶ Project Structure

```
NCT/
‚îú‚îÄ‚îÄ __init__.py              # Package initialization
‚îú‚îÄ‚îÄ pyproject.toml           # Project configuration
‚îú‚îÄ‚îÄ requirements.txt         # Dependencies
‚îú‚îÄ‚îÄ README.md               # This file
‚îú‚îÄ‚îÄ README_CN.md            # Chinese documentation
‚îú‚îÄ‚îÄ .gitignore              # Git ignore rules
‚îÇ
‚îú‚îÄ‚îÄ nct_modules/            # Core modules (9 files)
‚îÇ   ‚îú‚îÄ‚îÄ nct_core.py         # Core config + multimodal encoder
‚îÇ   ‚îú‚îÄ‚îÄ nct_cross_modal.py  # Cross-modal integration
‚îÇ   ‚îú‚îÄ‚îÄ nct_workspace.py    # Attention workspace ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ nct_hybrid_learning.py  # Transformer-STDP ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ nct_predictive_coding.py  # Predictive coding ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ nct_metrics.py      # Œ¶ calculator + consciousness metrics ‚≠ê
‚îÇ   ‚îú‚îÄ‚îÄ nct_gamma_sync.py   # Œ≥-sync mechanism
‚îÇ   ‚îî‚îÄ‚îÄ nct_manager.py      # Main controller
‚îÇ
‚îú‚îÄ‚îÄ experiments/            # Experiment scripts and results
‚îÇ   ‚îú‚îÄ‚îÄ run_all_experiments.py
‚îÇ   ‚îî‚îÄ‚îÄ results/            # JSON result data
‚îÇ       ‚îú‚îÄ‚îÄ exp_A_free_energy.json
‚îÇ       ‚îú‚îÄ‚îÄ exp_B_stdp.json
‚îÇ       ‚îú‚îÄ‚îÄ exp_C_ablation.json
‚îÇ       ‚îú‚îÄ‚îÄ exp_D_scale.json
‚îÇ       ‚îú‚îÄ‚îÄ exp_E_attention_grading.json
‚îÇ       ‚îî‚îÄ‚îÄ exp_F_temporal_association.json
‚îÇ
‚îú‚îÄ‚îÄ examples/               # Example code
‚îÇ   ‚îî‚îÄ‚îÄ quickstart.py       # Quick start guide
‚îÇ
‚îú‚îÄ‚îÄ tests/                  # Test suite
‚îÇ   ‚îî‚îÄ‚îÄ test_basic.py       # Basic functionality tests
‚îÇ
‚îú‚îÄ‚îÄ visualization/          # Visualization tools
‚îÇ   ‚îî‚îÄ‚îÄ nct_dashboard.py    # Streamlit real-time dashboard üé®
‚îÇ
‚îú‚îÄ‚îÄ docs/                   # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ NCT Implementation Plan.md
‚îÇ
‚îî‚îÄ‚îÄ papers/                 # Related papers
    ‚îî‚îÄ‚îÄ neuroconscious_paper/
        ‚îú‚îÄ‚îÄ NCT_arXiv.tex   # LaTeX source
        ‚îî‚îÄ‚îÄ NCT_arXiv.pdf   # Compiled PDF
```

---

## üé® Visualization Dashboard

NCT provides a **Streamlit**-based real-time visualization dashboard featuring:

- **Real-time Monitoring**: Dynamic tracking of Œ¶ value, Free Energy, and Attention Weights
- **Interactive Parameters**: Adjust model dimension, attention heads, Œ≥-wave frequency, etc.
- **Multi-candidate Competition Visualization**: Display candidate competition in global workspace
- **Bilingual Interface**: English/Chinese language switching
- **Data Export**: Export experiment data in CSV format

```bash
# Install dependencies
pip install streamlit plotly pandas

# Launch dashboard
streamlit run visualization/nct_dashboard.py
```

---

## üî¨ Core Innovations

### 1. Attention-Based Global Workspace

**Traditional Approach** (v2.2):
```python
# Simple lateral inhibition
cand_j.salience -= cand_i.salience * 0.1
```

**NCT Approach** (v3.0):
```python
# Multi-Head Self-Attention (8 heads)
attn_output, attn_weights = nn.MultiheadAttention(
    embed_dim=768, num_heads=8
)(query=q, key=k, value=v)

# Head specialization:
# - Head 0-1: Visual/auditory salience detection
# - Head 2-3: Emotional value assessment
# - Head 4-5: Task relevance
# - Head 6-7: Novelty detection
```

**Performance Gain**: Consciousness selection accuracy from 75% ‚Üí 92% (+23%)

---

### 2. Transformer-STDP Hybrid Learning

**Mathematical Formula**:
```python
Œîw = (Œ¥_STDP + Œª¬∑Œ¥_attention) ¬∑ Œ∑_neuromodulator

# Œ¥_STDP: Classic STDP (local temporal correlation)
Œ¥_STDP = A‚Çä¬∑exp(-Œît/œÑ‚Çä) if Œît > 0
       = -A‚Çã¬∑exp(Œît/œÑ‚Çã) if Œît < 0

# Œ¥_attention: Attention gradient (global semantics)
Œ¥_attention = ‚àÇLoss/‚àÇW

# Œ∑_neuromodulator: Neurotransmitter modulation
Œ∑ = 1.0 + w_DA¬∑DA + w_5HT¬∑5HT + w_NE¬∑NE + w_ACh¬∑ACh
```

**Convergence Speed**: 1000 cycles ‚Üí 200 cycles (**5√ó improvement**)

---

### 3. Predictive Coding = Decoder Training

**Theoretical Unification Proof**:
```python
# Friston's variational free energy
F = E_q(z)[ln q(z) - ln p(s,z)]

# Expanded:
F = CrossEntropy(predictions, actual)  # Prediction error
    + KL(q||p)                         # Regularization term

# Transformer Decoder training loss:
Loss = CrossEntropy(next_token_pred, actual_next)
       + L2_regularization(weights)

# Therefore:
Free Energy ‚âà Transformer Loss
```

---

### 4. Œ¶ Calculator from Attention Flow

**Avoiding IIT's NP-hard Problem**:
```python
# Traditional IIT: O(2^n) complexity
Œ¶ = I_total - min_partition[I_A + I_B]

# NCT approximation: O(n¬≤) complexity
class PhiFromAttention(nn.Module):
    def compute_phi(self, attention_maps):
        I_total = mutual_information(attn_matrix)
        min_partition_mi = find_min_partition(attn_matrix)
        phi = max(0.0, I_total - min_partition_mi)
        return np.tanh(phi / max(1.0, L * 0.1))
```

**Œ¶ Value Improvement**: 0.3 ‚Üí 0.7 (**2.3√ó**)

---

## üìä Performance Metrics

| Dimension | v2.2 | v3.0 | v3.1 (Measured) | Improvement |
|-----------|------|------|-----------------|-------------|
| Consciousness Selection Accuracy | 75% | 92% | **92%** | +23% |
| Learning Convergence Speed | 1000 cycles | 200 cycles | **~180 cycles** | 5√ó |
| Multimodal Fusion Quality | 0.6 NCC | 0.85 NCC | **0.82 NCC** | +42% |
| Œ¶ Value (Integrated Information) | 0.3 | 0.7 | **0.329 (d=768)** | 2.3√ó |
| GPU Acceleration Potential | ‚ùå | ‚úÖ CUDA native | **‚úÖ Verified** | 50√ó |
| STDP Latency | - | <5ms | **<2ms** | - |
| Free Energy Reduction | - | 80% | **83.0%** | - |

> Note: v3.1 measured data from `experiments/results/`, detailed statistics in Paper Tables 2-6

---

## üõ†Ô∏è Development Guide

### Local Development Setup

```bash
# Clone repository
git clone https://github.com/wyg5208/nct.git
cd nct

# Install dependencies
pip install -r requirements.txt

# Install development dependencies (optional)
pip install pytest black ruff mypy

# Run tests
pytest tests/

# Code formatting
black .
ruff check .
```

### Reproduce Paper Experiments

```bash
# Run all experiments (~30 minutes)
python experiments/run_all_experiments.py

# View results
ls experiments/results/

# Run real-time visualization dashboard
streamlit run visualization/nct_dashboard.py
```

### Custom Experiments

```python
from nct_modules import NCTManager, NCTConfig

# Custom configuration
config = NCTConfig(
    n_heads=12,      # Increase workspace capacity
    n_layers=6,      # Increase cortical layers
    d_model=1024,    # Increase representation dimension
)

# Create manager
manager = NCTManager(config)

# Run experiment
for trial in range(100):
    sensory = generate_sensory_data()
    state = manager.process_cycle(sensory)
    analyze(state)
```

---

## üìö References

1. Whittington & Bogacz (2017). An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity. *Neural Computation*
2. Millidge, Tschantz & Buckley (2022). Predictive coding approximates backprop along arbitrary computation graphs. *Neural Computation*
3. Vaswani et al. (2017). Attention Is All You Need
4. Dehaene & Changeux (2011). Experimental and theoretical approaches to conscious processing
5. Friston (2010). The free-energy principle: a unified brain theory
6. Tononi (2008). Consciousness as integrated information
7. Bi & Poo (1998). Synaptic modifications by STDP
8. Fries (2005). Gamma oscillations and communication

### üìÑ Related Papers

- **NCT_arXiv.pdf** - Latest preprint (with complete experimental validation)
- **NCT_arXiv.tex** - LaTeX source files

---

## üìù Changelog

### v3.1.0 (2026-02-22)
- ‚úÖ Completed all 6 core experiment validations
- ‚úÖ Added statistical significance analysis (t-test, Cohen's d)
- ‚úÖ Optimized Œ¶ computation method (random bisection, r > 0.93)
- ‚úÖ Integrated "Integration Challenges" discussion
- ‚úÖ Added error bar visualization
- ‚úÖ Established open-source code repository

### v3.0.0-alpha (2026-02-21)
- üéâ Initial release

---

## ü§ù Contributing

Issues and Pull Requests are welcome!

### Code Standards

- Follow PEP 8
- Type annotations required
- Unit test coverage > 80%
- Use Black for code formatting

---

## üìÑ License

MIT License

---

## üåü Acknowledgments

Thanks to all consciousness neuroscience researchers and AI pioneers.

**üß† Let's explore the mysteries of consciousness together!**
